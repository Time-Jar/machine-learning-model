{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Fetching Data from Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Import functions\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(project_root)\n",
    "from server import functions_aggregated, functions_supabase, functions_basic, functions_model\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "supabase = functions_supabase.auth()\n",
    "\n",
    "_acceptance_data, _actions_data, _app_names_data, _location_data, _sex, _weekdays, user_app_usage_data, users_data = functions_supabase.fetchTables(supabase)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df__acceptance, df__actions, df__app_names, df__location, df__sex, df__weekdays, df_user_app_usage, df_users = functions_basic.toPandasDataframes(_acceptance_data, _actions_data, _app_names_data, _location_data, _sex, _weekdays, user_app_usage_data, users_data)\n",
    "\n",
    "# Verify the structure of the dataframes\n",
    "df_user_app_usage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2 Data Preprocessing\n",
    "\n",
    "## 2.1 Remove uncompleted rows/entrys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_none_rows(df, column_name):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame where the specified column has 'None' or 'NaN'.\n",
    "    \"\"\"\n",
    "    return df.dropna(subset=[column_name])\n",
    "\n",
    "df_user_app_usage = remove_none_rows(df_user_app_usage, 'app_usage_time')\n",
    "\n",
    "# Verify the structure of the dataframes\n",
    "display(df_user_app_usage.head())\n",
    "display(df_user_app_usage.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Calculate/simplify data functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Normalize and numericalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_app_usage_normalized, df_users_normalized = functions_aggregated.normalizeAndNumericalize(df__acceptance, df__actions, df__app_names, df__location, df__sex, df__weekdays, df_user_app_usage, df_users)\n",
    "\n",
    "# num_acceptance_categories = df__acceptance['id'].nunique()\n",
    "\n",
    "# display(num_acceptance_categories)\n",
    "\n",
    "# Check the results\n",
    "display(df_user_app_usage_normalized.head())\n",
    "# display(df_user_app_usage_normalized.dtypes)\n",
    "\n",
    "display(df_users_normalized.head())\n",
    "# display(df_users_normalized.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = functions_aggregated.mergeUsersAndAppUsage(df_user_app_usage_normalized, df_users_normalized)\n",
    "\n",
    "display(merged_df.head())\n",
    "display(merged_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# merged_df is the DataFrame\n",
    "feature_columns = merged_df.columns.tolist()\n",
    "# Exclude 'should_be_blocked' from feature columns\n",
    "feature_columns = [col for col in merged_df.columns if col != 'should_be_blocked']\n",
    "display(feature_columns)\n",
    "\n",
    "model: Model = functions_model.build_and_compile_model(1000, 64, feature_columns, functions_model.SupervisedMLA.BINARY_CLASSIFICATION)\n",
    "model.summary()\n",
    "\n",
    "model.save('../model/model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "train, test = train_test_split(merged_df, test_size=0.1)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "# Prepare the data for the model, label_column is the column that we are trying to predict\n",
    "def prepare_data(df, feature_columns, label_column):\n",
    "    features = {col: df[col].values for col in feature_columns if col != label_column}\n",
    "    labels = df[label_column].values\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "x_train, y_train = prepare_data(train, feature_columns, 'should_be_blocked')\n",
    "x_val, y_val = prepare_data(val, feature_columns, 'should_be_blocked')\n",
    "x_test, y_test = prepare_data(test, feature_columns, 'should_be_blocked')\n",
    "\n",
    "# print(\"Train\")\n",
    "# display(x_train)\n",
    "# display(y_train)\n",
    "\n",
    "# print(\"Val\")\n",
    "# display(x_val)\n",
    "# display(y_val)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(x_val, y_val)\n",
    "print(\"-------------------------------------\")\n",
    "print(f'Validation Loss: {val_loss}')\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "# Predicting new data\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Since the output is now continuous, adjust how you interpret the predictions\n",
    "# For instance, you might round them to the nearest whole number or percentage\n",
    "predicted_values = predictions.flatten()  # If predictions need to be flattened\n",
    "display(predicted_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation mean absolute error values\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('Model Mean Absolute Error')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
