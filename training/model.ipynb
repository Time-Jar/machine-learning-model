{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Fetching Data from Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Import functions\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "sys.path.append(project_root)\n",
    "from server import functions_aggregated, functions_supabase, functions_basic, functions_model\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "supabase = functions_supabase.auth()\n",
    "\n",
    "_acceptance_data, _actions_data, _app_names_data, _location_data, _sex, _weekdays, user_app_usage_data, users_data = functions_supabase.fetchTables(supabase)\n",
    "\n",
    "display(user_app_usage_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df__acceptance, df__actions, df__app_names, df__location, df__sex, df__weekdays, df_user_app_usage, df_users = functions_basic.toPandasDataframes(_acceptance_data, _actions_data, _app_names_data, _location_data, _sex, _weekdays, user_app_usage_data, users_data)\n",
    "\n",
    "# Verify the structure of the dataframes\n",
    "df_user_app_usage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2 Data Preprocessing\n",
    "\n",
    "## 2.1 Remove uncompleted rows/entrys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_none_rows(df, column_name):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame where the specified column has 'None' or 'NaN'.\n",
    "    \"\"\"\n",
    "    return df.dropna(subset=[column_name])\n",
    "\n",
    "df_user_app_usage = remove_none_rows(df_user_app_usage, 'app_usage_time')\n",
    "\n",
    "# Verify the structure of the dataframes\n",
    "display(df_user_app_usage.head())\n",
    "display(df_user_app_usage.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Calculate/simplify data functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Normalize and numericalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_app_usage_normalized, df_users_normalized = functions_aggregated.normalizeAndNumericalize(df__acceptance, df__actions, df__app_names, df__location, df__sex, df__weekdays, df_user_app_usage, df_users)\n",
    "\n",
    "# num_acceptance_categories = df__acceptance['id'].nunique()\n",
    "\n",
    "# display(num_acceptance_categories)\n",
    "\n",
    "# Check the results\n",
    "display(df_user_app_usage_normalized.head())\n",
    "# display(df_user_app_usage_normalized.dtypes)\n",
    "\n",
    "display(df_users_normalized.head())\n",
    "# display(df_users_normalized.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = functions_aggregated.mergeUsersAndAppUsage(df_user_app_usage_normalized, df_users_normalized)\n",
    "\n",
    "display(merged_df.head())\n",
    "display(merged_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# merged_df is the DataFrame\n",
    "feature_columns = merged_df.columns.tolist()\n",
    "# Exclude 'should_be_blocked' from feature columns\n",
    "feature_columns = [col for col in merged_df.columns if col != 'should_be_blocked']\n",
    "display(feature_columns)\n",
    "\n",
    "# model: Model = functions_model.build_and_compile_model(1000, 64, feature_columns)\n",
    "model: Model = functions_model.build_and_compile_model_with_attention_machanism(1000, 64, feature_columns)\n",
    "model.summary()\n",
    "\n",
    "model.save('../model/model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "train, test = train_test_split(merged_df, test_size=0.1)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "# Prepare the data for the model, label_column is the column that we are trying to predict\n",
    "def prepare_data(df, feature_columns, label_column):\n",
    "    features = {col: df[col].values for col in feature_columns if col != label_column}\n",
    "    labels = df[label_column].values\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "x_train, y_train = prepare_data(train, feature_columns, 'should_be_blocked')\n",
    "x_val, y_val = prepare_data(val, feature_columns, 'should_be_blocked')\n",
    "x_test, y_test = prepare_data(test, feature_columns, 'should_be_blocked')\n",
    "\n",
    "# print(\"Train\")\n",
    "# display(x_train)\n",
    "# display(y_train)\n",
    "\n",
    "# print(\"Val\")\n",
    "# display(x_val)\n",
    "# display(y_val)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=200, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(x_val, y_val)\n",
    "print(\"-------------------------------------\")\n",
    "print(f'Validation Loss: {val_loss}')\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "# Predicting new data\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "predicted_values = predictions.flatten() \n",
    "display(predicted_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Training and validation loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# display(history.history)\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], 'b', label='Train Loss')  # Blue color\n",
    "plt.plot(history.history['val_loss'], 'r', label='Validation Loss')  # Red color\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], 'g', label='Train Accuracy')  # Green color\n",
    "plt.plot(history.history['val_accuracy'], 'm', label='Validation Accuracy')  # Magenta\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "true_labels = y_test\n",
    "predictions = [value > 0.5 for value in predicted_values]\n",
    "\n",
    "# display(true_labels)\n",
    "# display(predictions)\n",
    "\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 ROC Curve and AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute ROC curve and ROC area\n",
    "fpr, tpr, _ = roc_curve(y_test, predictions)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "\n",
    "# Calculate AUC\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, color='blue', lw=2, label='Precision-Recall curve (area = %0.2f)' % pr_auc)\n",
    "plt.fill_between(recall, precision, alpha=0.2, color='blue')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "def model_predict(data):\n",
    "    # Convert the data into the format expected by the model (dictionary of features)\n",
    "    # Data is a 2D array where each column corresponds to a feature\n",
    "    feature_dict = {feature: data[:, i] for i, feature in enumerate(x_train.keys())}\n",
    "    return model.predict(feature_dict)\n",
    "\n",
    "# Prepare a background dataset (a small subset or a mean/median representative)\n",
    "background_data = np.array([x_train[col][:100] for col in x_train.keys()]).T\n",
    "\n",
    "# Create the SHAP explainer with the custom prediction function\n",
    "explainer = shap.KernelExplainer(model_predict, background_data)\n",
    "\n",
    "# Calculate SHAP values for a subset of test data (for performance reasons)\n",
    "shap_values = explainer.shap_values(np.array([x_test[col][:100] for col in x_test.keys()]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, feature_names=list(x_train.keys()), plot_size=(15, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class KerasModelWrapper(BaseEstimator):\n",
    "    def __init__(self, model, feature_columns):\n",
    "        self.model = model\n",
    "        self.feature_columns = feature_columns\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_dict = {col: X[:, i] for i, col in enumerate(self.feature_columns)}\n",
    "        self.model.fit(X_dict, y, epochs=200, batch_size=32, verbose=0)\n",
    "        return self\n",
    "\n",
    "    def score(self, X, y):\n",
    "        X_dict = {col: X[:, i] for i, col in enumerate(self.feature_columns)}\n",
    "        y_pred = self.model.predict(X_dict).flatten()\n",
    "        return accuracy_score(y, y_pred > 0.5)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_dict = {col: X[:, i] for i, col in enumerate(self.feature_columns) if col != 'should_be_blocked'}\n",
    "        y_pred = self.model.predict(X_dict)\n",
    "        return (y_pred > 0.5).astype(int)  # Return binary labels\n",
    "\n",
    "\n",
    "# Create the wrapped model instance\n",
    "wrapped_model = KerasModelWrapper(model, feature_columns)\n",
    "\n",
    "# Now you can use learning_curve with the wrapped model\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    wrapped_model,\n",
    "    X=np.array([x_train[col] for col in x_train.keys() if col != 'should_be_blocked']).T,\n",
    "    y=y_train,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Calculate mean and standard deviation for training set scores\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "\n",
    "# Calculate mean and standard deviation for test set scores\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_sizes, train_mean, label='Training score', color='blue', marker='o')\n",
    "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, color='blue', alpha=0.15)\n",
    "\n",
    "plt.plot(train_sizes, test_mean, label='Cross-validation score', color='green', marker='s')\n",
    "plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, color='green', alpha=0.15)\n",
    "\n",
    "plt.title('Learning curve')\n",
    "plt.xlabel('Training Data Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7 Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## Get Predictions and Compare with Actual Labels\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_probs = model.predict(x_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "# Combine actual and predicted labels into a DataFrame\n",
    "error_analysis_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "\n",
    "# Add a column for correct/incorrect classification\n",
    "error_analysis_df['Correct'] = error_analysis_df['Actual'] == error_analysis_df['Predicted']\n",
    "\n",
    "## Analyze errors\n",
    "\n",
    "# False Positives\n",
    "false_positives = error_analysis_df[(error_analysis_df['Actual'] == 0) & (error_analysis_df['Predicted'] == 1)]\n",
    "\n",
    "# False Negatives\n",
    "false_negatives = error_analysis_df[(error_analysis_df['Actual'] == 1) & (error_analysis_df['Predicted'] == 0)]\n",
    "\n",
    "## Visualize the Errors\n",
    "\n",
    "x_test_df = pd.DataFrame(x_test)\n",
    "\n",
    "# Get indices of false positives and false negatives\n",
    "fp_indices = false_positives.index\n",
    "fn_indices = false_negatives.index\n",
    "\n",
    "# Iterate over each feature to create histograms\n",
    "for feature in x_test.keys():\n",
    "    # Extracting the feature values for false positives and false negatives\n",
    "    fp_feature_values = x_test_df.loc[fp_indices, feature]\n",
    "    fn_feature_values = x_test_df.loc[fn_indices, feature]\n",
    "\n",
    "    # Create histogram for the feature\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(fp_feature_values, alpha=0.5, bins=20, label='False Positives')\n",
    "    plt.hist(fn_feature_values, alpha=0.5, bins=20, label='False Negatives')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'Error Analysis for {feature}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
