{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Fetching Data from Supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supabase import create_client, Client\n",
    "import pandas as pd\n",
    "import os as os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize Supabase client\n",
    "url: str = os.environ.get(\"SUPABASE_PUBLIC_URL\")\n",
    "key: str = os.environ.get(\"SUPABASE_SERVICE_ROLE_KEY\")\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "# Fetch data from each table\n",
    "_acceptance_data = supabase.table(\"_acceptance\").select(\"*\").execute().data\n",
    "_actions_data = supabase.table(\"_actions\").select(\"*\").execute().data\n",
    "_app_names_data = supabase.table(\"_app_names\").select(\"*\").execute().data\n",
    "_location_data = supabase.table(\"_location\").select(\"*\").execute().data\n",
    "_sex = supabase.table(\"_sex\").select(\"*\").execute().data\n",
    "_weekdays = supabase.table(\"_weekdays\").select(\"*\").execute().data\n",
    "\n",
    "user_app_usage_data = supabase.table(\"user_app_usage\").select(\"*\").execute().data\n",
    "users_data = supabase.table(\"users\").select(\"*\").execute().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas DataFrames\n",
    "df__acceptance = pd.DataFrame(_acceptance_data)\n",
    "df__actions = pd.DataFrame(_actions_data)\n",
    "df__app_names = pd.DataFrame(_app_names_data)\n",
    "df__location = pd.DataFrame(_location_data)\n",
    "df__sex = pd.DataFrame(_sex)\n",
    "df__weekdays = pd.DataFrame(_weekdays)\n",
    "\n",
    "df_user_app_usage = pd.DataFrame(user_app_usage_data)\n",
    "df_users = pd.DataFrame(users_data)\n",
    "\n",
    "# Verify the structure of the dataframes\n",
    "df_user_app_usage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2 Data Preprocessing\n",
    "\n",
    "## 2.1 Remove uncompleted rows/entrys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_none_rows(df, column_name):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame where the specified column has 'None' or 'NaN'.\n",
    "    \"\"\"\n",
    "    return df.dropna(subset=[column_name])\n",
    "\n",
    "df_user_app_usage = remove_none_rows(df_user_app_usage, 'app_usage_time')\n",
    "\n",
    "# Verify the structure of the dataframes\n",
    "display(df_user_app_usage.head())\n",
    "display(df_user_app_usage.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Calculate/simplify data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Start functions\n",
    "\n",
    "def convert_boolean_to_numeric(df_original, column_name):\n",
    "    \"\"\"\n",
    "    Converts a boolean column in a DataFrame to 0 or 1.\n",
    "    \"\"\"\n",
    "    df = df_original.copy()\n",
    "\n",
    "    # Convert boolean to int (True to 1, False to 0)\n",
    "    df[column_name] = df[column_name].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def convert_string_to_date(df_original, dob_column):\n",
    "    \"\"\"\n",
    "    Converts a date of birth column from string to datetime and calculates the age.\n",
    "    \"\"\"\n",
    "    df = df_original.copy()\n",
    "    df[dob_column] = pd.to_datetime(df[dob_column])\n",
    "    df['age'] = df[dob_column].apply(\n",
    "        lambda dob: datetime.now().year - dob.year - ((datetime.now().month, datetime.now().day) < (dob.month, dob.day))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Final functions\n",
    "\n",
    "def normalize_numerical_data(df_original, column, fixed_max):\n",
    "    \"\"\"\n",
    "    Normalizes a specified column of the DataFrame.\n",
    "    \"\"\"\n",
    "    df = df_original.copy()\n",
    "\n",
    "    # Normalize the column\n",
    "    df[column] = df[column] / fixed_max\n",
    "    \n",
    "    # Ensure that the values do not exceed 1, more than 1 are clipped to 1\n",
    "    df[column] = df[column].clip(lower=0, upper=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def one_hot_encoding(df_original: pd.DataFrame, map_df: pd.DataFrame, column_to_encode: str, map_column: str, map_values: str):\n",
    "    \"\"\"\n",
    "    Maps a column to new values and applies one-hot encoding, ensuring all categories are represented.\n",
    "    \"\"\"\n",
    "    df = df_original.copy()\n",
    "    \n",
    "    # Map the column to new valuesthan\n",
    "    mapping = dict(zip(map_df[map_column], map_df[map_values]))\n",
    "    df[column_to_encode] = df[column_to_encode].map(mapping)\n",
    "\n",
    "    # One-hot encoding\n",
    "    df = pd.get_dummies(df, columns=[column_to_encode], prefix=column_to_encode)\n",
    "\n",
    "    # Add missing columns (if any) and fill with 0\n",
    "    required_columns = map_df[map_values].unique()\n",
    "    for col in required_columns:\n",
    "        full_col_name = f'{column_to_encode}_{col}'\n",
    "        if full_col_name not in df.columns:\n",
    "            df[full_col_name] = 0\n",
    "        df[full_col_name] = df[full_col_name].astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def normalize_time(df_original, time_column):\n",
    "    \"\"\"\n",
    "    Normalizes time values in a DataFrame column.\n",
    "    \"\"\"\n",
    "    df = df_original.copy()\n",
    "    \n",
    "    # Convert the time column to pandas datetime\n",
    "    df[time_column] = pd.to_datetime(df[time_column])\n",
    "\n",
    "    # Normalize time: hour + minute/60 + second/3600, then divide by 24\n",
    "    df[time_column] = df[time_column].apply(lambda x: (x.hour + x.minute / 60 + x.second / 3600) / 24)\n",
    "\n",
    "    return df\n",
    "\n",
    "import hashlib\n",
    "\n",
    "def hash_encode(df_original: pd.DataFrame, column: str, num_buckets: int):\n",
    "    \"\"\"\n",
    "    Hash-encodes a column in a DataFrame.\n",
    "    \"\"\"\n",
    "    df = df_original.copy()\n",
    "    \n",
    "    def hash_column(data):\n",
    "        return int(hashlib.sha256(str(data).encode()).hexdigest(), 16) % num_buckets\n",
    "\n",
    "    df[column] = df[column].apply(hash_column)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Normalize and numericalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply\n",
    "\n",
    "# df_user_app_usage\n",
    "df_user_app_usage_normalized = df_user_app_usage.drop(columns=['id', 'created_at'])\n",
    "df_user_app_usage_normalized = one_hot_encoding(df_user_app_usage_normalized, df__weekdays, 'weekday', 'id', 'weekday')\n",
    "df_user_app_usage_normalized = normalize_time(df_user_app_usage_normalized, 'time_of_day')\n",
    "df_user_app_usage_normalized = normalize_numerical_data(df_user_app_usage_normalized, 'app_usage_time', fixed_max=86400) # 24h max\n",
    "df_user_app_usage_normalized = hash_encode(df_user_app_usage_normalized, 'app_name', 1000) # hash-encoding upto 1000 apps\n",
    "df_user_app_usage_normalized = one_hot_encoding(df_user_app_usage_normalized, df__acceptance, 'acceptance', 'id', 'acceptance')\n",
    "df_user_app_usage_normalized = one_hot_encoding(df_user_app_usage_normalized, df__actions, 'action', 'id', 'action')\n",
    "df_user_app_usage_normalized = one_hot_encoding(df_user_app_usage_normalized, df__location, 'location', 'id', 'location')\n",
    "df_user_app_usage_normalized = convert_boolean_to_numeric(df_user_app_usage_normalized, 'should_be_blocked')\n",
    "\n",
    "# df_users\n",
    "df_users_normalized = convert_string_to_date(df_users, 'date_of_birth')\n",
    "df_users_normalized = df_users_normalized.drop(columns=['created_at', 'date_of_birth', 'first_name', 'last_name'])\n",
    "df_users_normalized = normalize_numerical_data(df_users_normalized, 'age', fixed_max=130) # max-age fixed to 130 years\n",
    "df_users_normalized = one_hot_encoding(df_users_normalized, df__sex, 'sex', 'id', 'sex')\n",
    "\n",
    "\n",
    "# num_acceptance_categories = df__acceptance['id'].nunique()\n",
    "\n",
    "# display(num_acceptance_categories)\n",
    "\n",
    "# Check the results\n",
    "display(df_user_app_usage_normalized.head())\n",
    "# display(df_user_app_usage_normalized.dtypes)\n",
    "\n",
    "display(df_users_normalized.head())\n",
    "# display(df_users_normalized.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dataframe_types(df_original):\n",
    "    \"\"\"\n",
    "    Reduces dataframe types.\n",
    "    \"\"\"\n",
    "    df = df_original.copy()\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            if col in [\"app_name\", \"user_id\"]:\n",
    "                df[col] = df[col].astype('uint16') # increase this if hash-encoding with larger bucket sizes\n",
    "            else:\n",
    "                df[col] = df[col].astype('bool') # all one-hot encoded things\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df =  df_users_normalized.merge(df_user_app_usage_normalized, left_on='id', right_on='user_id')\n",
    "merged_df = hash_encode(merged_df, 'user_id', 1000) # hash-encoding upto 1000 users\n",
    "\n",
    "merged_df = merged_df.drop(columns=['id'])\n",
    "merged_df = reduce_dataframe_types(merged_df)\n",
    "\n",
    "display(merged_df.head())\n",
    "display(merged_df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 TensorFlow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, Flatten, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_and_compile_model(num_buckets, embedding_dim, feature_columns):\n",
    "    \"\"\"\n",
    "    Builds and compiles a TensorFlow model for the given feature columns.\n",
    "    \"\"\"\n",
    "    # Input Layers\n",
    "    inputs = {col: Input(shape=(1,), name=col) for col in feature_columns}\n",
    "\n",
    "    # Embedding for hash-encoded columns\n",
    "    embeddings = []\n",
    "    for col in ['user_id', 'app_name']:\n",
    "        emb = Embedding(input_dim=num_buckets, output_dim=embedding_dim, input_length=1)(inputs[col])\n",
    "        emb = Flatten()(emb)\n",
    "        embeddings.append(emb)\n",
    "\n",
    "    # Directly use other columns\n",
    "    other_cols = [inputs[col] for col in feature_columns if col not in ['user_id', 'app_name']]\n",
    "    concatenated_features = Concatenate()(embeddings + other_cols)\n",
    "\n",
    "    # For simplicity, using Dense layers instead of Transformer\n",
    "    x = Dense(128, activation='relu')(concatenated_features)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    # Output Layer for Percentage (regression)\n",
    "    output = Dense(1, activation='linear')(x)  # 'linear' can be omitted as it is the default\n",
    "\n",
    "    # Model\n",
    "    model = Model(inputs=list(inputs.values()), outputs=output)\n",
    "\n",
    "    # Compile the model for regression\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])  # mae is mean absolute error\n",
    "\n",
    "    return model\n",
    "\n",
    "# merged_df is the DataFrame\n",
    "feature_columns = merged_df.columns.tolist()\n",
    "# Exclude 'should_be_blocked' from feature columns\n",
    "feature_columns = [col for col in merged_df.columns if col != 'should_be_blocked']\n",
    "display(feature_columns)\n",
    "\n",
    "model = build_and_compile_model(1000, 64, feature_columns)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting the data\n",
    "train, test = train_test_split(merged_df, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "# Prepare the data for the model, label_column is the column that we are trying to predict\n",
    "def prepare_data(df, feature_columns, label_column):\n",
    "    features = {col: df[col].values for col in feature_columns if col != label_column}\n",
    "    labels = df[label_column].values\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "\n",
    "x_train, y_train = prepare_data(train, feature_columns, 'should_be_blocked')\n",
    "x_val, y_val = prepare_data(val, feature_columns, 'should_be_blocked')\n",
    "x_test, y_test = prepare_data(test, feature_columns, 'should_be_blocked')\n",
    "\n",
    "# print(\"Train\")\n",
    "# display(x_train)\n",
    "# display(y_train)\n",
    "\n",
    "# print(\"Val\")\n",
    "# display(x_val)\n",
    "# display(y_val)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_accuracy = model.evaluate(x_val, y_val)\n",
    "print(\"-------------------------------------\")\n",
    "print(f'Validation Loss: {val_loss}')\n",
    "print(f'Validation Accuracy: {val_accuracy}')\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "# Predicting new data\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# Since the output is now continuous, adjust how you interpret the predictions\n",
    "# For instance, you might round them to the nearest whole number or percentage\n",
    "predicted_values = predictions.flatten()  # If predictions need to be flattened\n",
    "display(predicted_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation mean absolute error values\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')  # Change 'val_mae' according to what's in your history object\n",
    "plt.title('Model Mean Absolute Error')\n",
    "plt.ylabel('MAE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
